# Exercise 01: Understanding AI Bias

## Objective
Identify and explain bias in AI systems, then use AI strategically to deepen understanding.

**Important:** Complete Part A FIRST before using AI. The struggle is where learning happens!

---

## Part A: Build Your Foundation (NO AI FIRST)

### 1. Find or simulate a short example of model bias

**Type of bias I'm examining:**
<!-- TODO: Choose one type. Examples:
- Gender bias
- Racial bias
- Age bias
- Cultural bias
- Profession stereotyping
-->

**My example of this bias:**
<!-- TODO: Describe a real or hypothetical example. Ideas:
- AI image generator shows only men when asked for "CEO"
- Translation tool assumes doctors are male and nurses are female
- Hiring AI rejects resumes with names from certain ethnic backgrounds
- Facial recognition fails more often on dark-skinned people
- AI chatbot uses "he" by default when talking about engineers
-->

**Evidence or scenario demonstrating this bias:**
<!-- TODO: Be specific. Example:
I asked an AI image generator to "show me a software engineer" and it generated 8 images of men and 0 women. When I asked for "show me a nurse" it generated 7 women and 1 man. This shows gender stereotyping in professions.
-->

### 2. Explain in your own words why that bias may exist

**My explanation of the root cause:**
<!-- TODO: Think about WHY this happens. Examples:
- The training data had more pictures of male engineers because tech industry is male-dominated
- Historical data reflects past discrimination so AI learns and repeats it
- The people who created the dataset didn't include diverse examples
- Internet data contains societal stereotypes and AI learns from that
-->

**Where does this bias come from?**
<!-- TODO: Identify the source. Examples:
- Training data (what data was used?)
- Algorithm design (how was it built?)
- Human creators (who built it and what assumptions did they have?)
- Historical patterns (what past inequalities does it reflect?)
-->

### 3. Propose one mitigation (data balancing, prompt phrasing, model choice)

**My proposed solution:**
<!-- TODO: Think of a practical fix. Examples:
- Balance the training dataset to include equal numbers of men and women in all professions
- Add explicit instructions in prompts: "show diverse people of different genders and ethnicities"
- Use human reviewers to flag biased outputs before they're released
- Retrain the model with fairness constraints built in
- Add filters that catch stereotypical outputs
-->

**Why I think this would help:**
<!-- TODO: Explain your reasoning. Example:
If we balance the training data to show women as engineers and men as nurses, the AI will learn that any gender can have any profession. This breaks the stereotype at the source instead of just hiding it.
-->

**Potential limitations of my solution:**
<!-- TODO: Think critically. Examples:
- Getting balanced data might be hard if certain groups are underrepresented in real photos
- This fix works for images but might not work for text
- It might help with gender but not fix other biases like race or age
- It requires a lot of manual work to review and balance all the data
-->

---

## Part B: Strategic AI Use (AFTER completing Part A)

**Now that you've done your own analysis, use AI to deepen your understanding.**

### Questions I asked AI:

**Question 1:**
<!-- TODO: Ask AI to review YOUR analysis. Example:
"I found this bias example: [your example from Part A]. I think it exists because: [your explanation from Part A]. My proposed mitigation: [your solution from Part A]. What am I missing? What underlying causes haven't I considered?"
-->

**AI's response:**
<!-- TODO: Paste or summarize what AI told you -->

**What new insights did I gain:**
<!-- TODO: What did AI teach you that you didn't think of? Examples:
- AI explained that bias can happen at multiple stages not just training data
- I learned about intersectional bias (multiple biases combining)
- AI pointed out that my solution might create new problems I didn't think about
-->

---

**Question 2: Exploring edge cases**
<!-- TODO: Ask about real-world implications. Example:
"What happens when this bias appears in healthcare, hiring, or criminal justice? Here are my predictions: [your thoughts]. What are the real-world implications?"
-->

**AI's response:**
<!-- TODO: Paste or summarize what AI told you -->

**Real-world consequences I didn't consider:**
<!-- TODO: What harm could this bias cause? Examples:
- In healthcare: misdiagnosis for certain groups
- In hiring: qualified people denied jobs
- In criminal justice: wrong people arrested
- In loans: people denied credit unfairly
-->

---

**Question 3: Deeper understanding**
<!-- TODO: Ask about complexity. Example:
"Are there types of bias I haven't thought about? How do different biases interact with each other?"
-->

**AI's response:**
<!-- TODO: Paste or summarize what AI told you -->

**New types of bias I learned about:**
<!-- TODO: List any new types you discovered. Examples:
- Confirmation bias
- Selection bias
- Measurement bias
- Aggregation bias
-->

---

## Part C: Critical Reflection

### What % did you complete before using AI?
<!-- TODO: Be honest. Example:
I completed about 70% before using AI. I found my own example, explained why it happens, and proposed a solution. Then I used AI to check my thinking and learn more.
-->

### Did AI replace your thinking or amplify it?
<!-- TODO: Reflect honestly. Examples:
- AI amplified my thinking because I had my own ideas first and AI helped me go deeper
- AI replaced some of my thinking because I got lazy and just asked it to explain instead of thinking more myself
- AI amplified it - I still did the hard work but AI showed me angles I missed
-->

### Could you explain this to someone else without AI?
<!-- TODO: Test your understanding. Example:
Yes, I could explain my bias example, why it happens, and how to fix it without looking at AI responses. I understand it now because I thought through it myself first.

OR

Partially - I understand my own example but the deeper concepts AI explained I would need to review again.
-->

### What did you contribute that AI couldn't?
<!-- TODO: Think about your unique value. Examples:
- I found a specific example relevant to my experience
- I thought through the problem from my own perspective
- I connected it to things I've seen in real life
- I asked questions that mattered to me, not generic questions
-->

### Did the struggle of Part A help you learn more than if you'd asked AI immediately?
<!-- TODO: Compare the two approaches. Example:
Yes, definitely. If I had asked AI right away I would just read and forget. Because I struggled to think through it myself first, the concepts stuck in my brain better. When AI explained things I already thought about, I understood deeper instead of just reading words.
-->

### What would you do differently next time?
<!-- TODO: Plan for improvement. Examples:
- Next time I would spend even more time on my own analysis before using AI
- I would test my understanding by trying to explain it out loud before checking with AI
- I would ask AI more specific questions instead of general ones
- I would write down my predictions before asking AI so I can compare
-->

---

## Summary

**Key bias I learned about:**
<!-- TODO: One sentence. Example:
Gender bias in profession stereotyping happens because training data reflects historical inequalities.
-->

**Most important insight:**
<!-- TODO: What's the biggest thing you learned? Example:
Bias isn't just in the data - it can come from algorithm design, human assumptions, and how we measure success. Fixing bias requires addressing all these sources.
-->

**How this makes me a better developer:**
<!-- TODO: Connect to your work. Example:
Now I know to check if my training data is balanced and diverse. I also know to test my models on different groups to see if they work fairly for everyone. I can't just assume my code is fair - I have to test it.
-->
